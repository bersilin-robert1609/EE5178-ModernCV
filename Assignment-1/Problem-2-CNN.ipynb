{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "import torch.utils.data as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'mlp_cifar10_pytorch'\n",
    "PROJECT_ENTITY = 'cs20b013-bersilin'\n",
    "\n",
    "# Labels for the CIFAR-10 dataset\n",
    "\n",
    "LABELS = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform to a CIFAR image to a tensor of type float32\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CIFAR-10 dataset\n",
    "\n",
    "def load_data(transform: v2.Compose):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset\n",
    "    '''\n",
    "    train_data = datasets.CIFAR10(root='./data', \n",
    "                                  train=True,\n",
    "                                  download=True, \n",
    "                                  transform=transform)\n",
    "\n",
    "    test_data = datasets.CIFAR10(root='./data',\n",
    "                                 train=False,\n",
    "                                 download=True,\n",
    "                                 transform=transform)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Split the training set into a training and validation set\n",
    "\n",
    "def val_split(train_data, split=0.2, shuffle=True):\n",
    "    '''\n",
    "    Split the training set into a training and validation set\n",
    "\n",
    "    Args:\n",
    "    train_set: the training set\n",
    "    split: the proportion of the validation set\n",
    "    shuffle: whether to shuffle the indices before splitting\n",
    "    '''\n",
    "    train_size = len(train_data)\n",
    "    indices = list(range(train_size))\n",
    "    split = int(np.floor(split * train_size))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_set = dataloader.Subset(train_data, train_indices)\n",
    "    val_set = dataloader.Subset(train_data, val_indices)\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "# Create a dataloader\n",
    "\n",
    "def create_dataloader(train_set, val_set, test_set, batch_size):\n",
    "    '''\n",
    "    Create a dataloader for the training and test sets\n",
    "    '''\n",
    "    train_loader = dataloader.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = dataloader.DataLoader(val_set, batch_size=5 * batch_size, shuffle=False)\n",
    "    test_loader = dataloader.DataLoader(test_set, batch_size=5 * batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of all the sets passed\n",
    "\n",
    "def print_info(train_set, val_set, test_set):\n",
    "    '''\n",
    "    Print the size of the training, validation and test sets\n",
    "    '''\n",
    "    print(f\"Training set: {len(train_set)}\")\n",
    "    print(f\"Validation set: {len(val_set)}\")\n",
    "    print(f\"Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a random image from the dataset\n",
    "\n",
    "def show_random_image(dataset: datasets.CIFAR10, index: int = None):\n",
    "    '''\n",
    "    Shows a random image from the dataset\n",
    "    '''\n",
    "    if index is None:\n",
    "        index = torch.randint(0, len(dataset), (1,)).item()\n",
    "    else:\n",
    "        index = index\n",
    "                \n",
    "    image, label = dataset[index]\n",
    "    plt.imshow(image.permute(1, 2, 0)) # change the shape from (3, 32, 32) to (32, 32, 3)\n",
    "    plt.title(LABELS[label])\n",
    "    plt.show()\n",
    "\n",
    "    return index, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    '''\n",
    "    Plot the confusion matrix\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS.values())\n",
    "    disp.plot(cmap='Blues', xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(train_acc, val_acc):\n",
    "    '''\n",
    "    Plot the training and validation accuracies\n",
    "    '''\n",
    "    plt.plot(train_acc, label='Training accuracy')\n",
    "    plt.plot(val_acc, label='Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model - VGG11\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(CNN, self).__init__()\n",
    "        # input size: 3x32x32\n",
    "        self.sequence1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 64x16x16\n",
    "        self.sequence2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 128x8x8\n",
    "        self.sequence3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 256x4x4\n",
    "        self.sequence4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 512x2x2\n",
    "        self.sequence5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1, device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 512x1x1\n",
    "        self.fc1 = nn.Linear(512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sequence1(x)\n",
    "        x = self.sequence2(x)\n",
    "        x = self.sequence3(x)\n",
    "        x = self.sequence4(x)\n",
    "        x = self.sequence5(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    '''\n",
    "    Get the accuracy of the model on the data_loader\n",
    "    '''\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train(configs,\n",
    "          train_loader: dataloader.DataLoader,\n",
    "          val_loader: dataloader.DataLoader,\n",
    "          criterion: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          model: nn.Module):\n",
    "    \n",
    "    if configs['wandb_log']:\n",
    "        import wandb\n",
    "        run = wandb.init(project=PROJECT_NAME, entity=PROJECT_ENTITY, config=configs)\n",
    "        run.name = f\"lr={configs['learning_rate']}_bs={configs['batch_size']}_epochs={configs['num_epochs']}\"\n",
    "        wandb.watch(model, criterion, log='all')\n",
    "\n",
    "    print('Training the model...')\n",
    "    print('---------------------')\n",
    "\n",
    "    val_accuracies, train_accuracies = [], []\n",
    "\n",
    "    for epoch in range(configs['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_iterations = len(train_loader)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # change labels to one-hot encoding\n",
    "            # labels = nn.functional.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i != total_iterations-1):\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}', end='\\r')\n",
    "            else:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}')\n",
    "\n",
    "        print(f'Epoch {epoch + 1} done, Training Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        train_accuracy = get_accuracy(model, train_loader)\n",
    "        val_accuracy = get_accuracy(model, val_loader)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy} \\n')\n",
    "\n",
    "        if configs['wandb_log']:\n",
    "            wandb.log({'Epoch:': epoch + 1,\n",
    "                       'Training Loss': running_loss / len(train_loader),\n",
    "                       'Validation Loss': val_loss / len(val_loader),\n",
    "                       'Training Accuracy': train_accuracy,\n",
    "                       'Validation Accuracy': val_accuracy})\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    if configs['wandb_log']:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, configs, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(model, data_loader):\n",
    "    '''\n",
    "    Get the predicted labels of the model on the data_loader\n",
    "    '''\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            predicted_labels.append(predicted)\n",
    "\n",
    "    assert len(predicted_labels) == len(data_loader) # Check if all the data has been processed\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels, dim=0)\n",
    "\n",
    "    assert(predicted_labels.size(0) == len(data_loader.dataset)) # Check if the size of the predicted labels is equal to the size of the dataset\n",
    "\n",
    "    return predicted_labels.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "configs = {\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 10,\n",
    "    'momentum': 0.9,\n",
    "\n",
    "    'wandb_log': False,\n",
    "    'batch_norm': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = load_data(transform)\n",
    "train_set, val_set = val_split(train_set)\n",
    "train_loader, val_loader, test_loader = create_dataloader(train_set, val_set, test_set, configs['batch_size'])\n",
    "\n",
    "print_info(train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of one image is: \", train_set[0][0].size())\n",
    "print(\"The label of the first image is: \", LABELS[train_set[0][1]])\n",
    "index, label = show_random_image(train_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=configs['learning_rate'], momentum=configs['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, configs, train_accuracies, val_accuracies = train(configs, train_loader, val_loader, criterion, optimizer, model)\n",
    "plot_accuracies(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = get_accuracy(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "predicted_y = get_predicted_labels(model, test_loader)\n",
    "true_y = torch.tensor(test_loader.dataset.targets).to('cpu').numpy()\n",
    "\n",
    "plot_confusion_matrix(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, label = show_random_image(test_set)\n",
    "\n",
    "print(f'The true label is: {LABELS[label]}')\n",
    "print(f'The predicted label is: {LABELS[predicted_y[index]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
