{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "import torch.utils.data as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'mlp_cifar10_pytorch'\n",
    "PROJECT_ENTITY = 'cs20b013-bersilin'\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "ARCH = [500, 250, 100]\n",
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(mean, std):\n",
    "    '''\n",
    "    Returns a transform to convert a CIFAR image to a tensor of type float32\n",
    "    '''\n",
    "    return v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean, std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size: int, val_split: float = 0.2, shuffle: bool = True):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset\n",
    "    '''\n",
    "    train_data = datasets.CIFAR10(root=DATA_DIR, train=True, download=True)\n",
    "    test_data = datasets.CIFAR10(root=DATA_DIR, train=False, download=True)\n",
    "\n",
    "    mean = np.array(train_data.data).mean(axis=(0, 1, 2)) / 255\n",
    "    std = np.array(train_data.data).std(axis=(0, 1, 2)) / 255\n",
    "\n",
    "    transform = get_transform(mean, std)\n",
    "    train_data.transform = transform\n",
    "    test_data.transform = transform\n",
    "\n",
    "    train_size = int((1 - val_split) * len(train_data))\n",
    "    val_size = len(train_data) - train_size\n",
    "\n",
    "    train_data, val_data = dataloader.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "    train_loader = dataloader.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = dataloader.DataLoader(val_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = dataloader.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_data, test_data, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_image(dataset: datasets.CIFAR10, index: int = None):\n",
    "    '''\n",
    "    Shows a random image from the dataset\n",
    "    '''\n",
    "    if index is None:\n",
    "        index = np.random.randint(0, len(dataset))\n",
    "    else:\n",
    "        index = index\n",
    "                \n",
    "    image, label = dataset[index]\n",
    "    \n",
    "    plot = plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"True Label: {LABELS[label]}\")\n",
    "    plt.show()\n",
    "\n",
    "    return plot, index, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(train_acc, val_acc):\n",
    "    '''\n",
    "    Plot the training and validation accuracies\n",
    "    '''\n",
    "    plot = plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multi-layer perceptron model\n",
    "\n",
    "    Activation function: ReLU\n",
    "    Output activation function: Softmax\n",
    "    '''\n",
    "    def __init__(self, arch, in_size, out_size):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.sequence  = self.get_layers(arch, in_size, out_size)\n",
    "        self.fc = nn.Sequential(*self.sequence)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def get_layers(self, arch, in_size, out_size):\n",
    "        '''\n",
    "        Returns a list of layers for the model\n",
    "        '''\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features=in_size, out_features=arch[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(1, len(arch)):\n",
    "            layers.append(nn.Linear(in_features=arch[i-1], out_features=arch[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(in_features=arch[-1], out_features=out_size))\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model: nn.Module, data_loader: dataloader.DataLoader, device: torch.device):\n",
    "    '''\n",
    "    Get the accuracy of the model on the data_loader\n",
    "    '''\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(model: nn.Module, data_loader: dataloader.DataLoader, device: torch.device):\n",
    "    '''\n",
    "    Get the predicted labels of the model on the data_loader\n",
    "    '''\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            labels.append(predicted)\n",
    "\n",
    "    return torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train(configs, train_loader: dataloader.DataLoader, val_loader: dataloader.DataLoader, criterion: nn.CrossEntropyLoss,\n",
    "          optimizer: optim.Optimizer, model: nn.Module, device: torch.device):\n",
    "    '''\n",
    "    Train the model\n",
    "    '''\n",
    "    \n",
    "    print('Training the model...')\n",
    "    print('---------------------')\n",
    "\n",
    "    val_accuracies, train_accuracies = [], []\n",
    "\n",
    "    for epoch in range(configs['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_iterations = len(train_loader)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            loss = criterion(outputs, labels) # Calculate loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i != total_iterations-1):\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}', end='\\r')\n",
    "            else:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}')\n",
    "\n",
    "        print(f'Epoch {epoch + 1} done, Training Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        train_accuracy = get_accuracy(model, train_loader, device)\n",
    "        val_accuracy = get_accuracy(model, val_loader, device)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy} \\n')\n",
    "\n",
    "        if configs['wandb_log']:\n",
    "            wandb.log({'Epoch:': epoch + 1,\n",
    "                       'Training Loss': running_loss / len(train_loader),\n",
    "                       'Validation Loss': val_loss / len(val_loader),\n",
    "                       'Training Accuracy': train_accuracy,\n",
    "                       'Validation Accuracy': val_accuracy})\n",
    "\n",
    "    print('Finished Training')\n",
    "    print('---------------------')\n",
    "    \n",
    "    return model, configs, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'Validation Accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [50, 80, 100, 120, 150, 200]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.0005, 0.001, 0.003, 0.005, 0.007, 0.01, 0.03, 0.05]\n",
    "        },\n",
    "        'num_epochs': {\n",
    "            'values': [6, 8, 10, 12]\n",
    "        },\n",
    "        'batch_norm': {\n",
    "            'values': [False]\n",
    "        },\n",
    "        'momentum': {\n",
    "            'values': [0.87, 0.9, 0.93, 0.99]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME, entity=PROJECT_ENTITY)\n",
    "\n",
    "def train_sweep():\n",
    "    run = wandb.init()\n",
    "\n",
    "    configs = {\n",
    "        'num_epochs': wandb.config.num_epochs,\n",
    "        'batch_size': wandb.config.batch_size,\n",
    "        'learning_rate': wandb.config.learning_rate,\n",
    "        'batch_norm': wandb.config.batch_norm,\n",
    "        'momentum': wandb.config.momentum,\n",
    "\n",
    "        'wandb_log': True\n",
    "    }\n",
    "\n",
    "    run.name = f\"lr={configs['learning_rate']}_bs={configs['batch_size']}_epochs={configs['num_epochs']}_bn={configs['batch_norm']}_r{np.random.randint(0, 1000)}\"\n",
    "\n",
    "    model = MLP(ARCH, 3*32*32, 10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=configs['learning_rate'], momentum=configs['momentum'])\n",
    "    \n",
    "    wandb.watch(model, criterion, log='all')\n",
    "\n",
    "    train_data, test_data, train_loader, val_loader, test_loader = get_dataloader(configs['batch_size'])\n",
    "\n",
    "    model, configs, train_accuracies, val_accuracies = train(configs, train_loader, val_loader, criterion, optimizer, model, device)\n",
    "\n",
    "    test_accuracy = get_accuracy(model, test_loader, device)\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    if configs['wandb_log']:\n",
    "        wandb.log({'Test Accuracy': test_accuracy})\n",
    "        wandb.log({'confusion_matrix': wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                  y_true=test_data.targets,\n",
    "                                                                  preds=get_predicted_labels(model, test_loader, device).cpu().numpy(),\n",
    "                                                                  class_names=list(LABELS.values()))})\n",
    "        wandb.finish()\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train_sweep, count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
