{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "import torch.utils.data as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'mlp_cifar10_pytorch'\n",
    "PROJECT_ENTITY = 'cs20b013-bersilin'\n",
    "\n",
    "# Labels for the CIFAR-10 dataset\n",
    "\n",
    "LABELS = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform to a CIFAR image to a tensor of type float32\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CIFAR-10 dataset\n",
    "\n",
    "def load_data(transform: v2.Compose):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset\n",
    "    '''\n",
    "    train_data = datasets.CIFAR10(root='./data', \n",
    "                                  train=True,\n",
    "                                  download=True, \n",
    "                                  transform=transform)\n",
    "\n",
    "    test_data = datasets.CIFAR10(root='./data',\n",
    "                                 train=False,\n",
    "                                 download=True,\n",
    "                                 transform=transform)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Split the training set into a training and validation set\n",
    "\n",
    "def val_split(train_data, split=0.2, shuffle=True):\n",
    "    '''\n",
    "    Split the training set into a training and validation set\n",
    "\n",
    "    Args:\n",
    "    train_set: the training set\n",
    "    split: the proportion of the validation set\n",
    "    shuffle: whether to shuffle the indices before splitting\n",
    "    '''\n",
    "    train_size = len(train_data)\n",
    "    indices = list(range(train_size))\n",
    "    split = int(np.floor(split * train_size))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_set = dataloader.Subset(train_data, train_indices)\n",
    "    val_set = dataloader.Subset(train_data, val_indices)\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "# Create a dataloader\n",
    "\n",
    "def create_dataloader(train_set, val_set, test_set, batch_size):\n",
    "    '''\n",
    "    Create a dataloader for the training and test sets\n",
    "    '''\n",
    "    train_loader = dataloader.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = dataloader.DataLoader(val_set, batch_size=5 * batch_size, shuffle=False)\n",
    "    test_loader = dataloader.DataLoader(test_set, batch_size=5 * batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of all the sets passed\n",
    "\n",
    "def print_info(train_set, val_set, test_set):\n",
    "    '''\n",
    "    Print the size of the training, validation and test sets\n",
    "    '''\n",
    "    print(f\"Training set: {len(train_set)}\")\n",
    "    print(f\"Validation set: {len(val_set)}\")\n",
    "    print(f\"Test set: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a random image from the dataset\n",
    "\n",
    "def show_random_image(dataset: datasets.CIFAR10, index: int = None):\n",
    "    '''\n",
    "    Shows a random image from the dataset\n",
    "    '''\n",
    "    if index is None:\n",
    "        index = torch.randint(0, len(dataset), (1,)).item()\n",
    "    else:\n",
    "        index = index\n",
    "                \n",
    "    image, label = dataset[index]\n",
    "    plt.imshow(image.permute(1, 2, 0)) # change the shape from (3, 32, 32) to (32, 32, 3)\n",
    "    plt.title(LABELS[label])\n",
    "    plt.show()\n",
    "\n",
    "    return index, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    '''\n",
    "    Plot the confusion matrix\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS.values())\n",
    "    disp.plot(cmap='Blues', xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(train_acc, val_acc):\n",
    "    '''\n",
    "    Plot the training and validation accuracies\n",
    "    '''\n",
    "    plt.plot(train_acc, label='Training accuracy')\n",
    "    plt.plot(val_acc, label='Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model - VGG11\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # input size: 3x32x32\n",
    "        self.sequence1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 64x16x16\n",
    "        self.sequence2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 128x8x8\n",
    "        self.sequence3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 256x4x4\n",
    "        self.sequence4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 512x2x2\n",
    "        self.sequence5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # size = 512x1x1\n",
    "        self.fc1 = nn.Linear(512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input size: 3x32x32\n",
    "        x = self.sequence1(x)\n",
    "        print(x.size())\n",
    "        x = self.sequence2(x)\n",
    "        x = self.sequence3(x)\n",
    "        x = self.sequence4(x)\n",
    "        x = self.sequence5(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "        x = self.softmax(self.fc4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    '''\n",
    "    Get the accuracy of the model on the data_loader\n",
    "    '''\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train(configs,\n",
    "          train_loader: dataloader.DataLoader,\n",
    "          val_loader: dataloader.DataLoader,\n",
    "          criterion: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          model: nn.Module):\n",
    "    \n",
    "    if configs['wandb_log']:\n",
    "        import wandb\n",
    "        run = wandb.init(project=PROJECT_NAME, entity=PROJECT_ENTITY, config=configs)\n",
    "        run.name = f\"lr={configs['learning_rate']}_bs={configs['batch_size']}_epochs={configs['num_epochs']}\"\n",
    "        wandb.watch(model, criterion, log='all')\n",
    "\n",
    "    print('Training the model...')\n",
    "    print('---------------------')\n",
    "\n",
    "    val_accuracies, train_accuracies = [], []\n",
    "\n",
    "    for epoch in range(configs['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_iterations = len(train_loader)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # change labels to one-hot encoding\n",
    "            labels = nn.functional.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i != total_iterations-1):\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}', end='\\r')\n",
    "            else:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}')\n",
    "\n",
    "        print(f'Epoch {epoch + 1} done, Training Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        train_accuracy = get_accuracy(model, train_loader)\n",
    "        val_accuracy = get_accuracy(model, val_loader)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy} \\n')\n",
    "\n",
    "        if configs['wandb_log']:\n",
    "            wandb.log({'Epoch:': epoch + 1,\n",
    "                       'Training Loss': running_loss / len(train_loader),\n",
    "                       'Validation Loss': val_loss / len(val_loader),\n",
    "                       'Training Accuracy': train_accuracy,\n",
    "                       'Validation Accuracy': val_accuracy})\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    if configs['wandb_log']:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, configs, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(model, data_loader):\n",
    "    '''\n",
    "    Get the predicted labels of the model on the data_loader\n",
    "    '''\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            predicted_labels.append(predicted)\n",
    "\n",
    "    assert len(predicted_labels) == len(data_loader) # Check if all the data has been processed\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels, dim=0)\n",
    "\n",
    "    assert(predicted_labels.size(0) == len(data_loader.dataset)) # Check if the size of the predicted labels is equal to the size of the dataset\n",
    "\n",
    "    return predicted_labels.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "configs = {\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.00001,\n",
    "    'num_epochs': 10,\n",
    "    'momentum': 0,\n",
    "\n",
    "    'wandb_log': False,\n",
    "    'batch_norm': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train_set: datasets.CIFAR10, test_set: datasets.CIFAR10):\n",
    "    '''\n",
    "    Normalize the training and test sets\n",
    "    '''\n",
    "    mean = train_set.data.mean(axis=(0, 1, 2)) / 255\n",
    "    std = train_set.data.std(axis=(0, 1, 2)) / 255\n",
    "\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean, std, inplace=True)\n",
    "    ])\n",
    "\n",
    "    train_set.transform = transform\n",
    "    test_set.transform = transform\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training set: 40000\n",
      "Validation set: 10000\n",
      "Test set: 10000\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = load_data(transform)\n",
    "\n",
    "train_set, test_set = normalize(train_set, test_set)\n",
    "\n",
    "train_set, val_set = val_split(train_set)\n",
    "train_loader, val_loader, test_loader = create_dataloader(train_set, val_set, test_set, configs['batch_size'])\n",
    "\n",
    "print_info(train_set, val_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of one image is:  torch.Size([3, 32, 32])\n",
      "The label of the first image is:  airplane\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The size of one image is: \", train_set[0][0].size())\n",
    "print(\"The label of the first image is: \", LABELS[train_set[0][1]])\n",
    "index, label = show_random_image(train_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=configs['learning_rate'], momentum=configs['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (sequence1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (sequence2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (sequence3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (sequence4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (sequence5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  (fc4): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "---------------------\n",
      "torch.Size([256, 64, 16, 16])\n",
      "torch.Size([256, 64, 16, 16])s: 2.302452564239502\n",
      "torch.Size([256, 64, 16, 16])s: 2.302631139755249\n",
      "torch.Size([256, 64, 16, 16])s: 2.3026814460754395\n",
      "torch.Size([256, 64, 16, 16])s: 2.302615165710449\n",
      "torch.Size([256, 64, 16, 16])s: 2.3026788234710693\n",
      "torch.Size([256, 64, 16, 16])s: 2.3025197982788086\n",
      "torch.Size([256, 64, 16, 16])s: 2.302631378173828\n",
      "torch.Size([256, 64, 16, 16])s: 2.302687406539917\n",
      "torch.Size([256, 64, 16, 16])s: 2.302582263946533\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302581548690796\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024749755859375\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025717735290527\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3026318550109863\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3028109073638916\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302469253540039\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025009632110596\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024754524230957\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025550842285156\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302638292312622\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025965690612793\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025882244110107\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302332878112793\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302600383758545\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024630546569824\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025074005126953\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302577495574951\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3027255535125732\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302419662475586\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024649620056152\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3027210235595703\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025574684143066\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302640676498413\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302650213241577\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302582263946533\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3026747703552246\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302509307861328\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3026585578918457\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025050163269043\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3026983737945557\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302596092224121\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025834560394287\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024773597717285\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3027195930480957\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3023951053619385\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3027091026306152\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025856018066406\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302554130554199\n",
      "torch.Size([256, 64, 16, 16])ss: 2.30250883102417\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302687168121338\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302499771118164\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302722930908203\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025383949279785\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024516105651855\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302647829055786\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302687168121338\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302747964859009\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025054931640625\n",
      "torch.Size([256, 64, 16, 16])ss: 2.30246639251709\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302661657333374\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3023881912231445\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024215698242188\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3027238845825195\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3027329444885254\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302645206451416\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025758266448975\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025407791137695\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3028030395507812\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302544355392456\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024251461029053\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302381992340088\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3026881217956543\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302492141723633\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3025927543640137\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3024890422821045\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302619457244873\n",
      "torch.Size([256, 64, 16, 16])ss: 2.3026812076568604\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302621364593506\n",
      "torch.Size([256, 64, 16, 16])ss: 2.302480697631836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, configs, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plot_accuracies(train_accuracies, val_accuracies)\n",
      "Cell \u001b[0;32mIn[11], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(configs, train_loader, val_loader, criterion, optimizer, model)\u001b[0m\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 41\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m!=\u001b[39m total_iterations\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, configs, train_accuracies, val_accuracies = train(configs, train_loader, val_loader, criterion, optimizer, model)\n",
    "plot_accuracies(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = get_accuracy(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "predicted_y = get_predicted_labels(model, test_loader)\n",
    "true_y = torch.tensor(test_loader.dataset.targets).to('cpu').numpy()\n",
    "\n",
    "plot_confusion_matrix(true_y, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, label = show_random_image(test_set)\n",
    "\n",
    "print(f'The true label is: {LABELS[label]}')\n",
    "print(f'The predicted label is: {LABELS[predicted_y[index]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
