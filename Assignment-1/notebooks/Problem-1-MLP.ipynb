{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "import torch.utils.data as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'mlp_cifar10_pytorch'\n",
    "PROJECT_ENTITY = 'cs20b013-bersilin'\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "ARCH = [500, 250, 100]\n",
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(mean, std):\n",
    "    '''\n",
    "    Returns a transform to convert a CIFAR image to a tensor of type float32\n",
    "    '''\n",
    "    return v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean, std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size: int, val_split: float = 0.2, shuffle: bool = True):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset\n",
    "    '''\n",
    "    train_data = datasets.CIFAR10(root=DATA_DIR, train=True, download=True)\n",
    "    test_data = datasets.CIFAR10(root=DATA_DIR, train=False, download=True)\n",
    "\n",
    "    mean = np.array(train_data.data).mean(axis=(0, 1, 2)) / 255\n",
    "    std = np.array(train_data.data).std(axis=(0, 1, 2)) / 255\n",
    "\n",
    "    transform = get_transform(mean, std)\n",
    "    train_data.transform = transform\n",
    "    test_data.transform = transform\n",
    "\n",
    "    train_size = int((1 - val_split) * len(train_data))\n",
    "    val_size = len(train_data) - train_size\n",
    "\n",
    "    train_data, val_data = dataloader.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "    train_loader = dataloader.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = dataloader.DataLoader(val_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = dataloader.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_data, test_data, train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_image(dataset: datasets.CIFAR10, index: int = None):\n",
    "    '''\n",
    "    Shows a random image from the dataset\n",
    "    '''\n",
    "    if index is None:\n",
    "        index = np.random.randint(0, len(dataset))\n",
    "    else:\n",
    "        index = index\n",
    "                \n",
    "    image, label = dataset[index]\n",
    "    \n",
    "    plot = plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(\"True Label:\", LABELS[label])\n",
    "\n",
    "    return plot, index, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(train_acc, val_acc):\n",
    "    '''\n",
    "    Plot the training and validation accuracies\n",
    "    '''\n",
    "    plot = plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multi-layer perceptron model\n",
    "\n",
    "    Activation function: ReLU\n",
    "    Output activation function: Softmax\n",
    "    '''\n",
    "    def __init__(self, arch, in_size, out_size):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.sequence  = self.get_layers(arch, in_size, out_size)\n",
    "        self.fc = nn.Sequential(*self.sequence)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def get_layers(self, arch, in_size, out_size):\n",
    "        '''\n",
    "        Returns a list of layers for the model\n",
    "        '''\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features=in_size, out_features=arch[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(1, len(arch)):\n",
    "            layers.append(nn.Linear(in_features=arch[i-1], out_features=arch[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(in_features=arch[-1], out_features=out_size))\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model: nn.Module, data_loader: dataloader.DataLoader, device: torch.device):\n",
    "    '''\n",
    "    Get the accuracy of the model on the data_loader\n",
    "    '''\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(model: nn.Module, data_loader: dataloader.DataLoader, device: torch.device):\n",
    "    '''\n",
    "    Get the predicted labels of the model on the data_loader\n",
    "    '''\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            labels.append(predicted)\n",
    "\n",
    "    return torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train(configs, train_loader: dataloader.DataLoader, val_loader: dataloader.DataLoader, criterion: nn.CrossEntropyLoss,\n",
    "          optimizer: optim.Optimizer, model: nn.Module, device: torch.device):\n",
    "    '''\n",
    "    Train the model\n",
    "    '''\n",
    "    \n",
    "    print('Training the model...')\n",
    "    print('---------------------')\n",
    "\n",
    "    val_accuracies, train_accuracies = [], []\n",
    "\n",
    "    for epoch in range(configs['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_iterations = len(train_loader)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            loss = criterion(outputs, labels) # Calculate loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i != total_iterations-1):\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}', end='\\r')\n",
    "            else:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}')\n",
    "\n",
    "        print(f'Epoch {epoch + 1} done, Training Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        train_accuracy = get_accuracy(model, train_loader, device)\n",
    "        val_accuracy = get_accuracy(model, val_loader, device)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy} \\n')\n",
    "\n",
    "        if configs['wandb_log']:\n",
    "            wandb.log({'Epoch:': epoch + 1,\n",
    "                       'Training Loss': running_loss / len(train_loader),\n",
    "                       'Validation Loss': val_loss / len(val_loader),\n",
    "                       'Training Accuracy': train_accuracy,\n",
    "                       'Validation Accuracy': val_accuracy})\n",
    "\n",
    "    print('Finished Training')\n",
    "    print('---------------------')\n",
    "    \n",
    "    return model, configs, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'Validation Accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [50, 80, 100, 120, 150, 200]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [0.0005, 0.001, 0.003, 0.005, 0.007, 0.01, 0.03, 0.05]\n",
    "        },\n",
    "        'num_epochs': {\n",
    "            'values': [6, 8, 10, 12]\n",
    "        },\n",
    "        'batch_norm': {\n",
    "            'values': [False]\n",
    "        },\n",
    "        'momentum': {\n",
    "            'values': [0.87, 0.9, 0.93, 0.99]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 6mpnlgmb\n",
      "Sweep URL: https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/sweeps/6mpnlgmb\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME, entity=PROJECT_ENTITY)\n",
    "\n",
    "def train_sweep():\n",
    "    run = wandb.init()\n",
    "\n",
    "    configs = {\n",
    "        'num_epochs': wandb.config.num_epochs,\n",
    "        'batch_size': wandb.config.batch_size,\n",
    "        'learning_rate': wandb.config.learning_rate,\n",
    "        'batch_norm': wandb.config.batch_norm,\n",
    "        'momentum': wandb.config.momentum,\n",
    "\n",
    "        'wandb_log': True\n",
    "    }\n",
    "\n",
    "    run.name = f\"lr={configs['learning_rate']}_bs={configs['batch_size']}_epochs={configs['num_epochs']}_bn={configs['batch_norm']}_r{np.random.randint(0, 1000)}\"\n",
    "\n",
    "    model = MLP(ARCH, 3*32*32, 10).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=configs['learning_rate'], momentum=configs['momentum'])\n",
    "    \n",
    "    wandb.watch(model, criterion, log='all')\n",
    "\n",
    "    train_data, test_data, train_loader, val_loader, test_loader = get_dataloader(configs['batch_size'])\n",
    "\n",
    "    model, configs, train_accuracies, val_accuracies = train(configs, train_loader, val_loader, criterion, optimizer, model, device)\n",
    "\n",
    "    test_accuracy = get_accuracy(model, test_loader, device)\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    if configs['wandb_log']:\n",
    "        wandb.log({'Test Accuracy': test_accuracy})\n",
    "        wandb.log({'confusion_matrix': wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                  y_true=test_data.targets,\n",
    "                                                                  preds=get_predicted_labels(model, test_loader, device).cpu().numpy(),\n",
    "                                                                  class_names=list(LABELS.values()))})\n",
    "        wandb.finish()\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ow133fyv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_norm: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.87\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 12\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20b013\u001b[0m (\u001b[33mcs20b013-bersilin\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/bersi/Documents/Academic_Stuff/EE5178-ModernComputerVision/Assignment-1/notebooks/wandb/run-20240302_102618-ow133fyv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/runs/ow133fyv' target=\"_blank\">apricot-sweep-1</a></strong> to <a href='https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/sweeps/6mpnlgmb' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/sweeps/6mpnlgmb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/sweeps/6mpnlgmb' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/sweeps/6mpnlgmb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/runs/ow133fyv' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/runs/ow133fyv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training the model...\n",
      "---------------------\n",
      "Epoch 1, Iteration 200/200, Loss: 2.3011636734008796\n",
      "Epoch 1 done, Training Loss: 2.302084275484085\n",
      "Epoch 1, Validation Loss: 2.301416826248169\n",
      "Epoch 1, Training Accuracy: 0.155, Validation Accuracy: 0.1579 \n",
      "\n",
      "Epoch 2, Iteration 200/200, Loss: 2.2993307113647467\n",
      "Epoch 2 done, Training Loss: 2.300592374801636\n",
      "Epoch 2, Validation Loss: 2.2995911741256716\n",
      "Epoch 2, Training Accuracy: 0.177725, Validation Accuracy: 0.181 \n",
      "\n",
      "Epoch 3, Iteration 200/200, Loss: 2.2963976860046387\n",
      "Epoch 3 done, Training Loss: 2.2980638480186464\n",
      "Epoch 3, Validation Loss: 2.295721025466919\n",
      "Epoch 3, Training Accuracy: 0.17615, Validation Accuracy: 0.1774 \n",
      "\n",
      "Epoch 4, Iteration 200/200, Loss: 2.2757675647735596\n",
      "Epoch 4 done, Training Loss: 2.2905959832668303\n",
      "Epoch 4, Validation Loss: 2.2823442888259886\n",
      "Epoch 4, Training Accuracy: 0.16455, Validation Accuracy: 0.1645 \n",
      "\n",
      "Epoch 5, Iteration 200/200, Loss: 2.2550308704376224\n",
      "Epoch 5 done, Training Loss: 2.277521735429764\n",
      "Epoch 5, Validation Loss: 2.2715346574783326\n",
      "Epoch 5, Training Accuracy: 0.166725, Validation Accuracy: 0.1683 \n",
      "\n",
      "Epoch 6, Iteration 200/200, Loss: 2.2598142623901367\n",
      "Epoch 6 done, Training Loss: 2.2655331087112427\n",
      "Epoch 6, Validation Loss: 2.2554594898223876\n",
      "Epoch 6, Training Accuracy: 0.19525, Validation Accuracy: 0.1891 \n",
      "\n",
      "Epoch 7, Iteration 200/200, Loss: 2.2405755519866943\n",
      "Epoch 7 done, Training Loss: 2.2419522893428803\n",
      "Epoch 7, Validation Loss: 2.2288090991973877\n",
      "Epoch 7, Training Accuracy: 0.22675, Validation Accuracy: 0.2219 \n",
      "\n",
      "Epoch 8, Iteration 200/200, Loss: 2.1809492111206055\n",
      "Epoch 8 done, Training Loss: 2.2165831208229063\n",
      "Epoch 8, Validation Loss: 2.209851870536804\n",
      "Epoch 8, Training Accuracy: 0.235625, Validation Accuracy: 0.2334 \n",
      "\n",
      "Epoch 9, Iteration 200/200, Loss: 2.1674959659576416\n",
      "Epoch 9 done, Training Loss: 2.2018056082725526\n",
      "Epoch 9, Validation Loss: 2.1985567235946655\n",
      "Epoch 9, Training Accuracy: 0.244725, Validation Accuracy: 0.2431 \n",
      "\n",
      "Epoch 10, Iteration 200/200, Loss: 2.1513090133666997\n",
      "Epoch 10 done, Training Loss: 2.1920904576778413\n",
      "Epoch 10, Validation Loss: 2.1904726123809812\n",
      "Epoch 10, Training Accuracy: 0.25545, Validation Accuracy: 0.2547 \n",
      "\n",
      "Epoch 11, Iteration 200/200, Loss: 2.1806113719940186\n",
      "Epoch 11 done, Training Loss: 2.1845847642421723\n",
      "Epoch 11, Validation Loss: 2.183390908241272\n",
      "Epoch 11, Training Accuracy: 0.267925, Validation Accuracy: 0.2681 \n",
      "\n",
      "Epoch 12, Iteration 200/200, Loss: 2.2096145153045654\n",
      "Epoch 12 done, Training Loss: 2.1777846467494966\n",
      "Epoch 12, Validation Loss: 2.176808462142944\n",
      "Epoch 12, Training Accuracy: 0.2809, Validation Accuracy: 0.2779 \n",
      "\n",
      "Finished Training\n",
      "---------------------\n",
      "Test Accuracy: 0.2838\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch:</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Training Accuracy</td><td>▁▂▂▂▂▃▅▅▆▇▇█</td></tr><tr><td>Training Loss</td><td>███▇▇▆▅▃▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▂▂▁▂▃▅▅▆▇▇█</td></tr><tr><td>Validation Loss</td><td>███▇▆▅▄▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch:</td><td>12</td></tr><tr><td>Test Accuracy</td><td>0.2838</td></tr><tr><td>Training Accuracy</td><td>0.2809</td></tr><tr><td>Training Loss</td><td>2.17778</td></tr><tr><td>Validation Accuracy</td><td>0.2779</td></tr><tr><td>Validation Loss</td><td>2.17681</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">apricot-sweep-1</strong> at: <a href='https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/runs/ow133fyv' target=\"_blank\">https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch/runs/ow133fyv</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240302_102618-ow133fyv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_sweep, count=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
