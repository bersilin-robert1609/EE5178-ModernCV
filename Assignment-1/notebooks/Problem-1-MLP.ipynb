{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple MLP Baseline for Image classification for the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "import torch.utils.data as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for wandb sweeps\n",
    "PROJECT_NAME = 'mlp_cifar10_pytorch'\n",
    "PROJECT_ENTITY = 'cs20b013-bersilin'\n",
    "\n",
    "# The 10 classes in the CIFAR-10 dataset\n",
    "LABELS = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}\n",
    "\n",
    "# The architecture of the MLP model\n",
    "ARCH = [500, 250, 100]\n",
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(mean, std):\n",
    "    '''\n",
    "    Returns a transform to convert a CIFAR image to a tensor of type float32\n",
    "    '''\n",
    "    return v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean, std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size: int, val_split: float = 0.2, shuffle: bool = True):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset\n",
    "\n",
    "    Normalizes the data using the mean and standard deviation of the training data\n",
    "    '''\n",
    "    train_data = datasets.CIFAR10(root=DATA_DIR, train=True, download=True)\n",
    "    test_data = datasets.CIFAR10(root=DATA_DIR, train=False, download=True)\n",
    "\n",
    "    mean = np.array(train_data.data).mean(axis=(0, 1, 2)) / 255\n",
    "    std = np.array(train_data.data).std(axis=(0, 1, 2)) / 255\n",
    "\n",
    "    transform = get_transform(mean, std)\n",
    "    train_data.transform = transform\n",
    "    test_data.transform = transform\n",
    "\n",
    "    train_size = int((1 - val_split) * len(train_data))\n",
    "    val_size = len(train_data) - train_size\n",
    "\n",
    "    train_data, val_data = dataloader.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "    train_loader = dataloader.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = dataloader.DataLoader(val_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = dataloader.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_data, test_data, train_loader, val_loader, test_loader, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_image(dataset: datasets.CIFAR10, index: int = None, mean: np.ndarray = None, std: np.ndarray = None):\n",
    "    '''\n",
    "    Shows a random image from the dataset\n",
    "\n",
    "    If the mean and standard deviation are provided, the image is denormalized\n",
    "    If the index is provided, the image at that index is shown else a random image is shown\n",
    "    '''\n",
    "    if index is None:\n",
    "        index = np.random.randint(0, len(dataset))\n",
    "    else:\n",
    "        index = index\n",
    "                \n",
    "    image, label = dataset[index]\n",
    "\n",
    "    if mean is not None and std is not None:\n",
    "        # image is (3, 32, 32), std and mean are (3,)\n",
    "        image = image * std[:, None, None] + mean[:, None, None]\n",
    "    \n",
    "    plot = plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(f\"True Label: {LABELS[label]}\")\n",
    "    plt.show()\n",
    "\n",
    "    return plot, index, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(train_acc, val_acc):\n",
    "    '''\n",
    "    Plot the training and validation accuracies\n",
    "    '''\n",
    "    plot = plt.plot(train_acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multi-layer perceptron model with BatchNorm\n",
    "\n",
    "    Activation function: ReLU\n",
    "    Output activation function: Softmax\n",
    "    '''\n",
    "    def __init__(self, arch, in_size, out_size, batch_norm: bool = True):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.sequence  = self.get_layers(arch, in_size, out_size)\n",
    "        self.fc = nn.Sequential(*self.sequence)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def get_layers(self, arch, in_size, out_size):\n",
    "        '''\n",
    "        Returns a list of layers for the model\n",
    "        '''\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(in_features=in_size, out_features=arch[0]))\n",
    "        if self.batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(arch[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for i in range(1, len(arch)):\n",
    "            layers.append(nn.Linear(in_features=arch[i-1], out_features=arch[i]))\n",
    "            if self.batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(arch[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "        layers.append(nn.Linear(in_features=arch[-1], out_features=out_size))\n",
    "\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model: nn.Module, data_loader: dataloader.DataLoader, device: torch.device):\n",
    "    '''\n",
    "    Get the accuracy of the model on the data_loader\n",
    "    '''\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(model: nn.Module, data_loader: dataloader.DataLoader, device: torch.device):\n",
    "    '''\n",
    "    Get the predicted labels of the model on the data_loader\n",
    "    '''\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            labels.append(predicted)\n",
    "\n",
    "    return torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train(configs, train_loader: dataloader.DataLoader, val_loader: dataloader.DataLoader, criterion: nn.CrossEntropyLoss,\n",
    "          optimizer: optim.Optimizer, model: nn.Module, device: torch.device):\n",
    "    '''\n",
    "    Train the model\n",
    "    '''\n",
    "\n",
    "    if(configs['wandb_log']):\n",
    "        import wandb\n",
    "        wandb.init(project=PROJECT_NAME, entity=PROJECT_ENTITY)\n",
    "    \n",
    "    print('Training the model...')\n",
    "    print('---------------------')\n",
    "\n",
    "    val_accuracies, train_accuracies = [], []\n",
    "\n",
    "    for epoch in range(configs['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_iterations = len(train_loader)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs) # Forward pass\n",
    "            loss = criterion(outputs, labels) # Calculate loss\n",
    "            loss.backward() # Backward pass\n",
    "            optimizer.step() # Update weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i != total_iterations-1):\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}', end='\\r')\n",
    "            else:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}')\n",
    "\n",
    "        print(f'Epoch {epoch + 1} done, Training Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        train_accuracy = get_accuracy(model, train_loader, device)\n",
    "        val_accuracy = get_accuracy(model, val_loader, device)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy} \\n')\n",
    "\n",
    "        if configs['wandb_log']:\n",
    "            wandb.log({'Epoch:': epoch + 1,\n",
    "                       'Training Loss': running_loss / len(train_loader),\n",
    "                       'Validation Loss': val_loss / len(val_loader),\n",
    "                       'Training Accuracy': train_accuracy,\n",
    "                       'Validation Accuracy': val_accuracy})\n",
    "\n",
    "    print('Finished Training')\n",
    "    print('---------------------')\n",
    "    \n",
    "    return model, configs, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Wandb to run sweeps to find the best hyperparameters for the model from a set of hyperparameters.\n",
    "\n",
    "Link: https://wandb.ai/cs20b013-bersilin/mlp_cifar10_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameters found using wandb sweeps\n",
    "\n",
    "best_configs = {\n",
    "    'batch_norm': True,\n",
    "    'learning_rate': 0.007,\n",
    "    'num_epochs': 10,\n",
    "    'momentum': 0.87,\n",
    "    'wandb_log': False,\n",
    "    'batch_size': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_loader, val_loader, test_loader, mean, std = get_dataloader(best_configs['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a random image from the dataset\n",
    "\n",
    "plot, index, label = show_random_image(train_data, mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(ARCH, 3*32*32, 10, best_configs['batch_norm']).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=best_configs['learning_rate'], momentum=best_configs['momentum'])\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, configs, train_acc, val_acc = train(best_configs, train_loader, val_loader, criterion, optimizer, model, device)\n",
    "\n",
    "test_accuracy = get_accuracy(model, test_loader, device)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "plot_accuracies(train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "predicted_labels = get_predicted_labels(model, test_loader, device)\n",
    "true_labels = test_data.targets\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels.cpu().numpy())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS.values())\n",
    "disp.plot(cmap='Blues', xticks_rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an random image and its predicted label\n",
    "\n",
    "plot, index, label = show_random_image(test_data, mean=mean, std=std)\n",
    "\n",
    "predicted_label = predicted_labels[index].item()\n",
    "print(f'True Label: {LABELS[label]}')\n",
    "print(f'Predicted Label: {LABELS[predicted_label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
