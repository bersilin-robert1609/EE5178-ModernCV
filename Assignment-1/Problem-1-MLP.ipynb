{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bersi/Documents/Academic_Stuff/EE5178-ModernComputerVision/.conda/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "import torch.utils.data as dataloader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "PROJECT_NAME = 'mlp_cifar10_pytorch'\n",
    "PROJECT_ENTITY = 'cs20b013-bersilin'\n",
    "\n",
    "configs = {\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 10,\n",
    "    'momentum': 0.9,\n",
    "\n",
    "    'wandb_log': False,\n",
    "    'batch_norm': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs['wandb_log']:  \n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for the CIFAR-10 dataset\n",
    "\n",
    "LABELS = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform to a CIFAR image to a tensor of type float32\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the CIFAR-10 dataset\n",
    "\n",
    "def load_data(transform: v2.Compose):\n",
    "    '''\n",
    "    Load the CIFAR-10 dataset\n",
    "    '''\n",
    "    train_data = datasets.CIFAR10(root='./data', \n",
    "                                  train=True,\n",
    "                                  download=True, \n",
    "                                  transform=transform)\n",
    "\n",
    "    test_data = datasets.CIFAR10(root='./data',\n",
    "                                 train=False,\n",
    "                                 download=True,\n",
    "                                 transform=transform)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Split the training set into a training and validation set\n",
    "\n",
    "def val_split(train_data, split=0.2, shuffle=True):\n",
    "    '''\n",
    "    Split the training set into a training and validation set\n",
    "\n",
    "    Args:\n",
    "    train_set: the training set\n",
    "    split: the proportion of the validation set\n",
    "    shuffle: whether to shuffle the indices before splitting\n",
    "    '''\n",
    "    train_size = len(train_data)\n",
    "    indices = list(range(train_size))\n",
    "    split = int(np.floor(split * train_size))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "    train_set = dataloader.Subset(train_data, train_indices)\n",
    "    val_set = dataloader.Subset(train_data, val_indices)\n",
    "\n",
    "    return train_set, val_set\n",
    "\n",
    "# Create a dataloader\n",
    "\n",
    "def create_dataloader(train_set, val_set, test_set, batch_size):\n",
    "    '''\n",
    "    Create a dataloader for the training and test sets\n",
    "    '''\n",
    "    train_loader = dataloader.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = dataloader.DataLoader(val_set, batch_size=5 * batch_size, shuffle=False)\n",
    "    test_loader = dataloader.DataLoader(test_set, batch_size=5 * batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print size of training and test sets\n",
    "\n",
    "def print_info(train_X, test_X):\n",
    "    '''\n",
    "    Print the size of the training and test sets\n",
    "    '''\n",
    "    print(f'Training set: {len(train_X)}')\n",
    "    print(f'Test set: {len(test_X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a random image from the dataset\n",
    "\n",
    "def show_random_image(dataset: datasets.CIFAR10):\n",
    "    '''\n",
    "    Shows a random image from the dataset\n",
    "    '''\n",
    "    index = torch.randint(0, len(dataset), (1,)).item()\n",
    "    image, label = dataset[index]\n",
    "    plt.imshow(image.permute(1, 2, 0)) # change the shape from (3, 32, 32) to (32, 32, 3)\n",
    "    plt.title(LABELS[label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    '''\n",
    "    Plot the confusion matrix\n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=LABELS.values(), yticklabels=LABELS.values())\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture of the model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 500, device=device)\n",
    "        self.fc2 = nn.Linear(500, 250, device=device)\n",
    "        self.fc3 = nn.Linear(250, 100, device=device)\n",
    "        self.fc4 = nn.Linear(100, 10, device=device)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, 32 * 32 * 3) # Flatten the input tensor\n",
    "\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        \n",
    "        x = self.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    '''\n",
    "    Get the accuracy of the model on the data_loader\n",
    "    '''\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train(configs,\n",
    "          train_loader: dataloader.DataLoader,\n",
    "          val_loader: dataloader.DataLoader,\n",
    "          criterion: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          model: nn.Module):\n",
    "    \n",
    "    if configs['wandb_log']:\n",
    "        wandb.init(project=PROJECT_NAME, entity=PROJECT_ENTITY)\n",
    "        wandb.watch(model, criterion, log='all')\n",
    "\n",
    "    print('Training the model...')\n",
    "    print('---------------------')\n",
    "\n",
    "    for epoch in range(configs['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_iterations = len(train_loader)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch + 1}, Iteration {i + 1}/{total_iterations}, Loss: {loss.item()}', end='\\r')\n",
    "\n",
    "        print(f'Epoch {epoch + 1} done, Training Loss: {running_loss / len(train_loader)}')\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "        train_accuracy = get_accuracy(model, train_loader)\n",
    "        val_accuracy = get_accuracy(model, val_loader)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "        if configs['wandb_log']:\n",
    "            wandb.log({'Epoch:': epoch + 1,\n",
    "                       'Training Loss': running_loss / len(train_loader),\n",
    "                       'Validation Loss': val_loss / len(val_loader),\n",
    "                       'Training Accuracy': train_accuracy,\n",
    "                       'Validation Accuracy': val_accuracy})\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    if configs['wandb_log']:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model, configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(model, data_loader):\n",
    "    '''\n",
    "    Get the predicted labels of the model on the data_loader\n",
    "    '''\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            _, predicted = torch.max(preds, 1)\n",
    "            predicted_labels.append(predicted)\n",
    "\n",
    "    assert len(predicted_labels) == len(data_loader) # Check if all the data has been processed\n",
    "\n",
    "    predicted_labels = torch.cat(predicted_labels, dim=0)\n",
    "\n",
    "    assert(predicted_labels.size(0) == len(data_loader.dataset)) # Check if the size of the predicted labels is equal to the size of the dataset\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = load_data(transform)\n",
    "train_set, val_set = val_split(train_set)\n",
    "train_loader, val_loader, test_loader = create_dataloader(train_set, val_set, test_set, configs['batch_size'])\n",
    "\n",
    "model = MLP(device).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=configs['learning_rate'], momentum=configs['momentum'])\n",
    "\n",
    "model, configs = train(configs, train_loader, val_loader, criterion, optimizer, model)\n",
    "\n",
    "test_accuracy = get_accuracy(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "predicted_y = get_predicted_labels(model, test_loader)\n",
    "true_y = torch.tensor(test_loader.dataset.targets)\n",
    "\n",
    "plot_confusion_matrix(true_y, predicted_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
